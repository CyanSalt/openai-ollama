import { createServer } from 'node:http'
import type { App } from 'h3'
import { createApp, createRouter, defineEventHandler, getHeaders, readBody, readValidatedBody, toNodeListener } from 'h3'
import { ofetch } from 'ofetch'
import { z } from 'zod'
import { OpenAIOllamaStream } from './transform'

export interface ModelObject {
  id: string,
  name?: string,
}

export interface BackendOptions {
  apiKey: string,
  baseURL?: string,
  models?: (string | ModelObject)[],
}

export interface Backend {
  app: App,
}

export function prepare(options: BackendOptions): Backend {

  const {
    apiKey,
    baseURL,
    models,
  } = options

  const app = createApp()

  const router = createRouter()
  app.use(router)

  // Health check
  router.get(
    '/',
    defineEventHandler(() => {
      return 'Active.'
    }),
  )

  const prefix = baseURL
    ? baseURL.replace(/\/$/, '')
    : 'https://api.openai.com/v1'

  router.use(
    '/v1/chat/**',
    defineEventHandler(async event => {
      const body = await readBody(event)
      const response = await ofetch(`${prefix}/chat/${event.context.params!._}`, {
        method: event.method,
        headers: {
          ...getHeaders(event),
          // Must be lowercase to override event.headers
          authorization: `Bearer ${apiKey}`,
        },
        body,
        responseType: 'stream',
      })
      return response
    }),
  )

  const modelObjects = models
    ? models.map(model => {
      return typeof model === 'string'
        ? { id: model, name: model }
        : model
    })
    : models

  router.get(
    '/v1/models',
    modelObjects
      ? defineEventHandler(() => {
        return {
          object: 'list',
          data: modelObjects.map(model => {
            return {
              id: model.id,
              object: 'model',
              created: Date.now(),
              owned_by: 'openai-ollama',
            }
          }),
        }
      })
      : defineEventHandler(async event => {
        const body = await readBody(event)
        const response = await ofetch(`${prefix}/models`, {
          method: event.method,
          headers: {
            ...getHeaders(event),
            Authorization: `Bearer ${apiKey}`,
          },
          body,
        })
        return response
      }),
  )

  router.use(
    '/api/chat',
    defineEventHandler(async (event) => {
      const body = await readBody(event)
      const model = modelObjects?.find(
        item => item.id === body.model || item.name === body.model,
      )?.id ?? body.model
      const response = await ofetch(`${prefix}/chat/completions`, {
        method: event.method,
        headers: {
          authorization: `Bearer ${apiKey}`,
        },
        body: {
          ...body,
          model,
        },
        responseType: 'stream',
      })
      return response.pipeThrough(new OpenAIOllamaStream() as never)
    }),
  )

  router.post(
    '/api/show',
    modelObjects
      ? defineEventHandler(async event => {
        const schema = z.object({
          model: z.string(),
        })
        const {
          model,
        } = await readValidatedBody(event, schema.parse)
        const modelObject = modelObjects.find(item => item.id === model)
        if (!modelObject) {
          return undefined // to respond 404
        }

        // https://github.com/ollama/ollama/blob/main/docs/api.md?file=api.md#response-26
        return {
          modelfile: `# Modelfile generated by "openai-ollama"\n`,
          parameters: '',
          template: '',
          details: {
            parent_model: '',
            format: '',
            family: 'llama',
            families: ['llama'],
            parameter_size: 'Unknown',
            quantization_level: 'Unknown',
          },
          model_info: {
            'general.architecture': 'llama',
            'general.file_type': 2,
            'general.parameter_count': 0,
            'general.quantization_version': 0,
            'llama.attention.head_count': 0,
            'llama.attention.head_count_kv': 0,
            'llama.attention.layer_norm_rms_epsilon': 0.00001,
            'llama.block_count': 0,
            'llama.context_length': 0,
            'llama.embedding_length': 0,
            'llama.feed_forward_length': 0,
            'llama.rope.dimension_count': 0,
            'llama.rope.freq_base': 0,
            'llama.vocab_size': 0,
            'tokenizer.ggml.bos_token_id': 0,
            'tokenizer.ggml.eos_token_id': 0,
            'tokenizer.ggml.model': 'gpt2',
          },
          capabilities: [],
        }
      })
      : defineEventHandler(() => undefined),
  )

  router.get(
    '/api/tags',
    modelObjects
      ? defineEventHandler(() => {
        return {
          models: modelObjects.map(model => {
            return {
              model: model.id,
              name: model.name || model.id,
              modified_at: new Date().toISOString(),
              size: 0,
              digest: '',
              details: {
                format: '',
                family: 'llama',
                families: ['llama'],
                parameter_size: 'Unknown',
                quantization_level: 'Unknown',
              },
            }
          }),
        }
      })
      : defineEventHandler(async event => {
        const body = await readBody(event)
        const response = await ofetch(`${prefix}/models`, {
          method: event.method,
          headers: {
            ...getHeaders(event),
            Authorization: `Bearer ${apiKey}`,
          },
          body,
        })
        const { data } = response.data
        return {
          models: data.map(model => ({
            model: model.id,
            name: model.name || model.id,
            modified_at: new Date().toISOString(),
            size: 0,
            digest: '',
            details: {
              format: '',
              family: 'llama',
              families: ['llama'],
              parameter_size: 'Unknown',
              quantization_level: 'Unknown',
            },
          })),
        }
      }),
  )

  return {
    app,
  }

}

export interface ServerOptions {
  port: number,
}

export function serve(backend: Backend, options: ServerOptions) {
  const { app } = backend
  const { port } = options

  const listener = toNodeListener(app)
  const server = createServer(listener)
  return new Promise<void>(resolve => {
    server.listen(port, resolve)
  })
}
